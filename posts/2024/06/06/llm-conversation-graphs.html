<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cemrekarakas.com/static/css/theme.css">
    <link rel="icon" type="image/png" href="https://emoji.dutl.uk/png/64x64/ðŸ§‘ðŸ»â€ðŸ’».png">
    <title>'Concept: LLM conversation graphs'</title>
    
    <meta property="og:title" name="title" content="'Concept: LLM conversation graphs'" />
    <meta property="og:image" name="image" content="https://cemrekarakas.com/posts/2024/06/06/cherry-blossoms-in-blue.png" />
    <meta property="og:description" name="description" content="Nowadays the mainstream method of managing our conversations with LLMs is a linear chat structure where, every question-answer pair is followed by another...." />
    <meta property="og:type" content="website" />
    <meta property="og:url" name="url" content="https://cemrekarakas.com/posts/2024/06/06/llm-conversation-graphs">
    <meta property="twitter:title" name="title" content="'Concept: LLM conversation graphs'" />
    <meta property="twitter:image" name="image" content="https://cemrekarakas.com/posts/2024/06/06/cherry-blossoms-in-blue.png" />
    <link rel="canonical" href="https://cemrekarakas.com/posts/2024/06/06/llm-conversation-graphs" />
    
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D53MYFV2RS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-D53MYFV2RS');
</script>
<script>
  // Function to handle the click event on headers.
  function handleClick(event) {
      // Get the element's ID.
      const id = this.id;

      // Define the base URL.
      const baseUrl = window.location.href.split('#')[0];

      // Construct the URL with ID.
      const urlWithId = baseUrl + '#' + id;

      // Create a temporary text area element to facilitate copying to the clipboard.
      const textArea = document.createElement('textarea');
      textArea.value = urlWithId;
      document.body.appendChild(textArea);

      // Select and copy the URL with ID to the clipboard.
      textArea.select();
      document.execCommand('copy');

      // Remove the temporary text area element.
      document.body.removeChild(textArea);

      alert("Link copied to clipboard.");
  }

  // Add a DOMContentLoaded event listener to run the code when the document is ready.
  document.addEventListener('DOMContentLoaded', function() {
      // Add a click event listener to the specified header elements (h2, h3, h4).
      document.querySelectorAll('h2, h3, h4').forEach(function(header) {
          header.addEventListener('click', handleClick);
      });
  });
</script>

<p><meta name="twitter:card" content="summary_large_image"/>
<meta content='@cemreefe' name='twitter:site'/>
<meta content='@cemreefe' name='twitter:creator'/></p>
  </head>
  <body>
    <div class="wrapper">
      <div class="content">
        <nav>
          <h3 id="cemres-blog">Cemre's Blog</h3>
<p><a href="/">About</a>
<a href="/blog">Blog</a>
<a href="/contact">Contact</a>
<a href="/archive">Archive</a></p>
<hr />
        </nav>
      </div>
      <div class="content">
        <main>
          
          <div class="categoryTag">
            software
          </div>
          
          <div class="categoryTag">
            llm
          </div>
          
          <div class="categoryTag">
            concept
          </div>
          
          <h1 id="managing-llm-conversations-using-a-graph-structure">Managing LLM Conversations Using a Graph Structure</h1>
<p><img alt="Cherry blossoms in blue by Van Gogh, depicting a cherry blossom branch with its many branchettes." src="./cherry-blossoms-in-blue.png" /></p>
<h2 id="introduction">Introduction</h2>
<p>Nowadays the mainstream method of managing our conversations with LLMs is a <strong>linear chat structure</strong> where, every question-answer pair is followed by another.
If we would like to refer to a previous answer with its full context, we often feel the need to redact everything after that point to continue our conversation from a particular point in the discussion, <strong>losing our access to the other branch</strong> (<em>Ã  la</em> github copilot) or we have <strong>isolated branches</strong> where we can continue our conversation, where we need to switch views (<em>Ã  la</em> ChatGPT). </p>
<p>Either way, <strong>we lose the ability to branch out and then recollect our conversation</strong> as is the case with most naturally ocurring human dialogue where parties may get <strong>sidelined</strong> for a while <strong>and get back to the topic</strong> at hand.</p>
<p>To overcome this, I am making a <strong>simple proposal of graph-managed LLM conversations</strong>, where we make use if different branches to explore different parts of our conversation and then can <strong>merge contexts</strong> into one to continue our conversation with the full picture.</p>
<h2 id="setting-the-glossary">Setting the glossary</h2>
<h3 id="the-conversation-node">The conversation node</h3>
<p>In a contemporary human-LLM conversation, the most basic building block of a dialogue is a <strong>question-answer (or input-output) pair</strong>. For simplicity, we will call this a conversation node. </p>
<p>A conversation node can be defined as an input-output pair in the human-llm conversation.</p>
<p><img alt="A conversation node, input and output pair" src="./1-conversation-node.png" /></p>
<h3 id="conversation-history">Conversation history</h3>
<p>A <strong>sequence of conversation nodes</strong> form a conversation history. A conversation history can be authentic (formed by consecutive pairs of real input-output) or fabricated, meaning the data is synthetic and the conversation has not taken place as is represented.</p>
<p><img alt="A sequence of conversation nodes" src="./2-conversation-history.png" /></p>
<h3 id="transforming-conversation-history-into-api-input">Transforming Conversation History into API Input</h3>
<p>The sequence of conversation nodes are generally dumped into a json file to feed an LLM API with, this, as is the case with the conversation history, is <strong>open to manipulation</strong>.</p>
<p><img alt="Conversion of conversation history to llm input, a sequence of nodes and the corresponding json structure (pseudocode)" src="./3-llm-input.png" /></p>
<h3 id="branching-conversations">Branching Conversations</h3>
<p>When we branch off from an earlier node, we may lose context from the existing branch when talking to the LLM agent. Which is expected.</p>
<p><img alt="Branching off and losing context" src="./4-branching-off.png" /></p>
<p>However the conversation may pick up pace in the new branch to explore some unfamiliar concept. Eventually, the user is going to want to return to the "main" conversation, which has its context isolated from the current conversational branch.</p>
<p><img alt="Conversation ongoing in two branches" src="./5-two-branches-contd.png" /></p>
<h2 id="proposal">Proposal</h2>
<h3 id="merging-priorities">Merging Priorities</h3>
<p>When we have two or more branches in a conversation, we may want to merge them back into a single conversation. However, one important aspect to consider is the order in which these branches are merged. The order of merging can affect the context that the LLM has when generating responses. </p>
<p>Tthe user should have the ability to specify the order of merging, effectively setting the priority of each branch. At this point, we can fabricate a new conversation history where the branch of an unprioritized context can precede the prioritized one.</p>
<p><img alt="Merging branches with priorities" src="./6-merging-priorities.png" /></p>
<h3 id="background-summarization-and-prompt-compression">Background Summarization and Prompt Compression</h3>
<p>During the merging of the branches, the LLM will have a lot of context to consider when generating responses, depending on the length of every individual branch, the unprioritized context can fall far behind. To help the LLM manage this context effectively and increase recall, we can use techniques such as background summarization to rebase deprioritized branch closer to the prioritized branch and/or try prompt compression to achieve the same.</p>
<p>One such method for prompt compression is the LLMLingua (add reference here), which provides a structured approach to managing and compressing LLM prompts.</p>
<p><img alt="Background summarization and prompt compression" src="./7-background-summarization.png" /></p>
<h3 id="conclusion">Conclusion</h3>
<p>As a concept, graph-managed LLM conversations can enable us to hold broader discussions with specific sides of a topic and then return to the main topic with the complete context of all discussions which is a very powerful function of human-to-human conversation. This approach will greatly improve the context-awareness of your LLM agent and should give you a means of having all-round discussions with it with more precision and depth.</p>
<div class="commentbox"></div>
<script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
<script>commentBox('5631895295492096-proj')</script>

<div onclick="{window.open('https://twitter.com/intent/tweet?url=Check out this \''+ document.getElementsByTagName('h1')[0].innerHTML +'\' article on Cemre\'s Blog: '+window.location.href, '_blank');}" 
    style="
    position: fixed;
    bottom: 0;
    right: 1em;
    cursor: pointer;">
  <img src="https://cdn.iconscout.com/icon/free/png-256/twitter-share-button-3289861-2758559.png" style="width: 5em;" alt="Share to Twitter">
</div>
        </main>
      </div>
      <footer>
        <p>2023 - Created with <a href="https://github.com/cemreefe/SimplyMarkdown">SimplyMarkdown</a></p>
      </footer>
    </div>
  </body>
</html>